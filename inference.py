"""
 Gary Plunkett
 Code for wavenet inference. 
 Test files should have been generated by preprocess_maestro script
"""

import os
import json
import csv
import math
from scipy.io.wavfile import write
import numpy as np
import torch
import torch.nn.functional as F
import utils
import debug
from nn.discretized_mix_logistics import SampleDiscretizedMixLogistics

import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt


def inference(test_dir, model_filename, device, output_dir,
              use_conditioning,  use_logistic_mix,
              teacher_force=False, teacher_length=-1, use_train_mode=False,
              no_pedal=False, audio_hz=16000, generation_length=4):

    """
    Run inference on midi data. Test data should be generated by preprocess_maestro in test mode.

    - Setting use_train_mode=True will output teacher forced train-time audio (fast model quality check)

    - Setting teacher_force=True will use teacher forcing in inference mode. This is useful in conjunction
          with setting teacher_length less than teacher_audio's length. Using teacher forcing, then switching
          to autoregressive mode shortly after, gives autoregression a good history of samples to start with.

    - Use no_pedal to remove pedal data. Midi data saved with pedal information, but if excluded from training
          make surre to set it here too.
    """
    
    model = torch.load(model_filename)['model'].to(device)
    
    # canvas to plot midi roll on
    fig, ax = plt.subplots()

    # Open the metadata csv and get file information
    metadata = csv.DictReader(open(test_dir + "filenames.csv"))
    for filedata in metadata:
        filename = filedata["index"]

        if use_conditioning:
            midiX = torch.load(test_dir + '/' + filename + ".midiX")
            if no_pedal:
                midiX = midiX[:88]
        
            # plot midi features
            plt.cla()
            ax.spy(midiX, markersize=3, aspect="auto", origin='lower')
            plt.savefig(output_dir + '/' + filename + "_midiroll.png")

            midiX = midiX.unsqueeze(0).to(device)
        else:
            midiX = None
            
        if use_train_mode or teacher_force:
            teacher_audio = torch.load(test_dir + '/' + filename + ".audioX")
            teacher_audio = teacher_audio.unsqueeze(0).to(device)

        if use_train_mode:
            filename = filename + "_trainmode"
            if use_logistic_mix:
                sampler = SampleDiscretizedMixLogistics()
            else:
                sampler = utils.CategoricalSampler()
            model_output = model((midiX, teacher_audio), training=False)
            # FLAG get rid of [0] by changing wavenet autoencoder output
            audio = sampler(model_output)[0]

        elif teacher_force:
            filename = filename + "_teacherforce"            
            # if specified teacher_length less than teacher_audio's length, trim it
            if (teacher_length > 0) and ((teacher_length*audio_hz) < teacher_audio.size(-1)):
                teacher_samples = int(teacher_length*audio_hz)
                teacher_audio = teacher_audio[:, :teacher_samples]
                filename = filename + str(teacher_length) + "s"
            audio = model.inference(midiX, use_logistic_mix=use_logistic_mix,
                                    teacher_audio=teacher_audio)

        else:
            if use_conditioning:
                audio = model.inference(midiX, use_logistic_mix)
            else:
                audio = model.inference(midiX, use_logistic_mix, length=generation_length, device=device, batch_size=1)

        audio = audio.squeeze().cpu().numpy()
        write(output_dir + '/' + filename + "_inference.wav", audio_hz, audio)

        print("Saved " + filename)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str, help="Location of configuration file")   
    args = parser.parse_args()

    with (open(args.config)) as f:
        data = f.read()
    config = json.loads(data)["inference_config"]
    
    inference(**config)
